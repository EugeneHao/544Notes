[["index.html", "544 Notes Introduction", " 544 Notes JHao Sun 2022-06-12 Introduction This is a notebook for STAT 544, instructed by Dr. Jarad Niemi. The notes are based on the class notes from this webpage and should only be used for personal study. "],["probabilty-and-inference.html", "Chapter 1 Probabilty and Inference", " Chapter 1 Probabilty and Inference Bayes’ Rule: If \\(A\\) and \\(B\\) are events in event space \\(F\\), then Bayes’ rule states that \\[ P(A\\mid B) = \\frac{P(B\\mid A)P(A)}{P(B)} = \\frac{P(B\\mid A)P(A)}{P(B\\mid A)P(A) + P(B\\mid A^c)P(A^c)} \\] Let \\(y\\) be the data we will collect from an experiment, \\(K\\) be everything we know for certain about the world (aside from \\(y\\)), and \\(\\theta\\) be anything we don’t know for certain. A Bayesian statistician is an individual who makes decisions based on the probability distribution of those things we don’t know conditional on what we know, i.e. \\(p(\\theta\\mid y, K)\\). Parameter estimation: \\(p(\\theta\\mid y, M)\\), where \\(M\\) is the model with parameter vector \\(\\theta\\) Hypothesis testing: \\(p(M_j\\mid y, M)\\) Prediction: \\(p(\\tilde{y}\\mid y, M)\\) Parameter Estimation Example: exponential model Let \\(Y\\mid \\theta \\sim Exp(\\theta)\\), the likelihood is \\(p(y\\mid \\theta) = \\theta \\exp(-\\theta y)\\). Let’s assume a prior \\(\\theta \\sim Ga(a, b)\\), \\(p(\\theta) = \\frac{b^a}{\\Gamma(a)}\\theta^{a-1}e^{-b\\theta}\\), then prior predictive distribution is \\[ p(y)=\\int p(y \\mid \\theta) p(\\theta) d \\theta=\\frac{b^{a}}{\\Gamma(a)} \\frac{\\Gamma(a+1)}{(b+y)^{a+1}} \\] The posterior is \\[ p(\\theta\\mid y) = \\frac{p(y\\mid \\theta)p(\\theta)}{p(y)} = \\frac{(b+y)^{a+1}}{\\Gamma(a+1)}\\theta^{a+1-1}e^{-(b+y)\\theta} \\] thus \\(\\theta\\mid y \\sim Ga(a+1, b+y)\\). If \\(p(y) &lt; \\infty\\), we can use \\(p(\\theta\\mid y) \\propto p(y\\mid \\theta)p(\\theta)\\) to find the posterior. In the example, \\(\\theta^{a}e^{-(b+y)\\theta}\\) is the kernel of a \\(Ga(a+1, b+y)\\) distribution. Bayesian learning: \\(p(\\theta) \\rightarrow p(\\theta\\mid y_1) \\rightarrow p(\\theta\\mid y_1, y_2) \\rightarrow \\ldots\\) Model selection Formally, to select a model, we use \\(p(M_j\\mid y) \\propto p(y\\mid M_j)p(M_j)\\). Thus, a Bayesian approach provides a natural way to learn about models, i.e. \\(p(M_j) \\rightarrow p(M_j\\mid y)\\). Prediction \\(p(\\tilde{y}\\mid y) = \\int p(\\tilde{y}, \\theta \\mid y)dy = \\int p(\\tilde{y}\\mid \\theta)p(\\theta\\mid y)d\\theta\\). From the previous example, let \\(y_i \\sim Exp(\\theta)\\), \\(\\theta \\sim Ga(a,b)\\), \\[ \\begin{aligned} p(\\tilde{y} \\mid y) &amp;=\\int p(\\tilde{y} \\mid \\theta) p(\\theta \\mid y) d \\theta \\\\ &amp;=\\int \\theta e^{-\\theta \\tilde{y}} \\frac{(b+n \\bar{y})^{a+n}}{\\Gamma(a+1)} \\theta^{a+n-1} e^{-\\theta(b+n \\bar{y})} d \\theta \\\\ &amp;=\\frac{(b+n \\bar{y})^{a+n}}{\\Gamma(a+n)} \\int \\theta^{a+n+1-1} e^{-\\theta(b+n \\bar{y}+\\tilde{y})} d \\theta \\\\ &amp;=\\frac{(b+n \\bar{y})^{a+n}}{\\Gamma(a+n)} \\frac{\\Gamma(a+n+1)}{(b+n \\bar{y}+\\tilde{y})^{a+n+1}} \\\\ &amp;=\\frac{(a+n)(b+n \\bar{y})^{a+n}}{(\\tilde{y}+b+n \\bar{y})^{a+n+1}} \\end{aligned} \\] which is called the Lomax distribution for \\(\\tilde{y}\\) with parameters \\(a + n\\) and \\(b + n\\bar y\\). Probabilty: A subjective probability describes an individual’s personal judgement about how likely a particular event is to occur. Rational individuals can differ about the probability of an event by having different knowledge, i.e. \\(P(E \\mid K_1) \\neq P(E \\mid K_2)\\). But given enough data, we might have \\(P(E \\mid K_1,Y) \\approx P(E \\mid K_2, y)\\). "],["parameter-estimation.html", "Chapter 2 Parameter Estimation", " Chapter 2 Parameter Estimation Suppose \\(Y \\sim Bin(n, \\theta)\\), and \\(\\theta \\sim Be(a, b)\\), then \\(p(\\theta\\mid y) \\propto \\theta^{a + y - 1}(1-\\theta)^{b+n-y-1}\\). Thus \\(\\theta\\mid y \\sim Be(a+y, b+n-y)\\). A \\(100(1 - \\alpha)\\%\\) credible interval is any interval in the posterior that contains the parameter with probability \\((1- \\alpha)\\). Define a loss function \\(L(\\theta, \\hat\\theta) = -U(\\theta, \\hat\\theta)\\) where \\(\\theta\\) is the parameter of interest and \\(\\hat\\theta = \\hat\\theta(y)\\) is the estimator of \\(\\theta\\). Find the estimator that minimizes the expected loss: \\(\\hat\\theta_{Bayes} = \\arg\\min_{\\hat\\theta}E\\left[ L(\\theta, \\hat\\theta)\\mid y\\right]\\). Common estimators are: Mean: \\(\\hat\\theta_{Bayes} = E(\\theta\\mid y)\\) minimizes \\(L(\\theta, \\hat\\theta) = (\\theta, \\hat\\theta)^2\\). Median: \\(\\int_{\\hat\\theta}^\\infty p(\\theta\\mid y) d\\theta = \\frac{1}{2}\\) minimizes \\(L(\\theta, \\hat\\theta) = |\\theta - \\hat\\theta |\\). Mode: \\(\\hat\\theta_{Bayes} = \\arg\\max_{\\theta} p(\\theta\\mid y)\\) is obtained by minimizing \\(L(\\theta, \\hat\\theta) = -I(|\\theta- \\hat\\theta) &lt; \\epsilon)\\) as \\(\\epsilon \\rightarrow 0\\), also called maximum a posterior (MAP) estimator. A \\(100(1- \\alpha)\\%\\) credible interval is any interval \\((L, U)\\) such that \\(1 - \\alpha = \\int_{L}^U p(\\theta\\mid y) d\\theta\\). Some typical intervals are Equal-tailed: \\(\\alpha/2 = \\int_{-\\infty}^L p(\\theta\\mid y)d\\theta = \\int_{U}^\\infty p(\\theta\\mid y)d\\theta\\). One-sided: either \\(L = -\\infty\\) or \\(U = \\infty\\) Highest posterior density (HPD): \\(p(L\\mid y) = p(U\\mid y)\\) A prior probability distribution, often called simply the prior, of an uncertain quantity \\(\\theta\\) is the probability distribution that would express one’s uncertainty about \\(\\theta\\) before the “data” is taken into account. A prior \\(p(\\theta)\\) is conjugate if for \\(p(\\theta) \\in \\mathcal{P}\\) and \\(p(y\\mid \\theta) \\in \\mathcal{F}\\), \\(p(\\theta\\mid y) \\in \\mathcal{P}\\) where \\(\\mathcal{F}\\) and $ are families of distributions. Example: the beta distribution (\\(\\mathcal{P}\\)) is conjugate to the binomial distribution Discrete priors are conjugate. Mixture of conjugate priors are conjugate. A natural conjugate prior is a conjugate prior that has the same functional form as the likelihood. Example: the beta distribution is a natural conjugate prior since \\(p(\\theta) \\propto \\theta^{a-1}(1-\\theta)^{b-1}\\) and \\(L(\\theta) \\propto \\theta^y(1-\\theta)^{n-y}\\). Mixture of conjugate priors: Suppose \\(\\theta \\sim \\sum_{i = 1}^I p_ip_i(\\theta)\\), \\(\\sum_{i=1}^I p_i = 1\\) and \\(p_i(y) = \\int p(y\\mid \\theta) p_i(\\theta)d\\theta\\), then \\(\\theta\\mid y \\sim \\sum_{i=1}^I p_i&#39;p_i(\\theta\\mid y)\\), \\(p_i&#39; \\propto p_ip_i(y)\\) where \\(p_i(\\theta\\mid y) = p(y \\mid \\theta)p_i(\\theta)/p_i(y)\\). Example: \\(Y \\sim Bin(n, \\theta)\\), \\(\\theta \\sim pBe(a_1, b_1) + (1-p)Be(a_2, b_2)\\), then \\[ \\theta\\mid y \\sim p&#39;Be(a_1 + y, b_1 + n - y) + (1 - p&#39;)Be(a_2 + y, b_2 + n - y) \\] with \\(p&#39; = \\frac{pp_1(y)}{pp_1(y) + (1-p)p_2(y)}\\), \\(p_i(y) = {n \\choose y} \\frac{Beta(a_i + y, b_i + n - y)}{Beta(a_i, b_i)}\\). A default prior is used when a data analyst is unable or unwilling to specify an informative prior distribution. Suppose we use \\(\\phi = \\log (\\theta/(1-\\theta))\\), the log odds of the parameter, and set \\(p(\\phi) \\propto 1\\), then the implied prior on \\(\\theta\\) is \\[ \\begin{aligned} p_{\\theta}(\\theta) \\propto &amp; 1\\left|\\frac{d}{d \\theta} \\log (\\theta /[1-\\theta])\\right| \\\\ &amp;=\\frac{1-\\theta}{\\theta}\\left[\\frac{1}{1-\\theta}+\\frac{\\theta}{[1-\\theta]^{2}}\\right] \\\\ &amp;=\\frac{1-\\theta}{\\theta}\\left[\\frac{[1-\\theta]+\\theta}{[1-\\theta]^{2}}\\right] \\\\ \\rightarrow \\quad &amp;=\\theta^{-1}[1-\\theta]^{-1} \\end{aligned} \\] a \\(Be(0, 0)\\) if that were a proper distribution, and is different from setting \\(p(\\theta) \\propto 1\\) which results in the \\(Be(1,1)\\) prior. Jeffreys prior is a prior that is invariant to parameterization and is obtained via \\[ p(\\theta) \\propto \\sqrt{\\text{det} \\space \\mathcal{I}(\\theta)} \\] where \\(\\mathcal{I}(\\theta)\\) is the Fisher information. Example: for a binomial distribution \\(\\mathcal{I}(\\theta) = \\frac{n}{\\theta(1-\\theta)}\\) so \\(p(\\theta) \\propto \\theta^{-1/2} (1-\\theta)^{-1/2}\\). Improper prior: An unnormalized density, \\(f(\\theta)\\), is proper if \\(\\int f(\\theta)d\\theta = c &lt; \\infty\\), and otherwise it is improper. Example: \\(Be(0, 0)\\) is an improper distribution. Suppose \\(Y \\sim Bin(n, \\theta)\\), the posterior \\(\\theta\\mid y \\sim Be(y, n-y)\\) is proper if \\(0 &lt; y &lt; n\\). Normal Distribution Thm: If \\(Y_i \\stackrel{iid}{\\sim} N(\\mu, s^2)\\) (\\(s^2\\) is known), Jeffreys prior for \\(\\mu\\) is \\(p(\\mu) \\propto 1\\) and the posterior is \\[ p(\\mu \\mid y) \\propto \\exp\\left(-\\frac{1}{2s^2/n} [\\mu^2 - 2\\mu\\bar y] \\right) \\sim N(\\bar y, s^2/n) \\] The natural conjugate prior is \\(\\mu \\sim N(m, C)\\) and the posterior \\(\\mu\\mid y \\sim N(m&#39;, C&#39;)\\) where \\(C&#39; = [\\frac{1}{C} + \\frac{n}{s^2}]^{-1}\\) and \\(m&#39; = C&#39;[\\frac{m}{C} + \\frac{n}{s^2}\\bar y]\\). The posterior precision is the sum of the prior and observation precisions The posterior mean is a precision weighted average of the prior and data. Jeffrey prior/posterior are the limits of the conjugate prior/posterior as \\(C \\rightarrow \\infty\\), i.e. \\(\\lim_{C\\rightarrow \\infty} N(m, C) \\stackrel{d}{\\rightarrow} \\propto 1\\) and \\(\\lim_{C \\rightarrow \\infty} N(m&#39;, C&#39;) \\stackrel{d}{\\rightarrow} N(\\bar y, s^2/n)\\). Thm: If \\(Y_i \\stackrel{iid}{\\sim} N(m, \\sigma^2)\\) (\\(m\\) is known), Jeffery prior for \\(\\sigma^2\\) is \\(p(\\sigma^2) \\propto 1/\\sigma^2\\) and the posterior is \\[ p(\\sigma^2\\mid y) \\propto (\\sigma^2)^{-n/2-1}\\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n [y_i - m]^2 \\right) \\sim IG(n/2, \\sum_{i=1}^n [y_i - m]^2). \\] The natural conjugate prior is \\(\\sigma^2 \\sim IG(a, b)\\) and the posterior \\(\\sigma\\mid y \\sim IG(a + n/2, b + \\sum_{i=1}^n [y_i - m]^2/2)\\). JAGS # Y ~ Bin(n, theta), theta ~ Be(1, 1), we observe Y = 3 successes out of 10 attempts library(rjags) model = &quot; model { y ~ dbin(theta,n) # notice p then n theta ~ dbeta(a,b) }&quot; dat = list(n=10, y=3, a=1, b=1) m = jags.model(textConnection(model), dat) r = coda.samples (m, &quot;theta&quot;, n.iter=1000) summary(r) plot(r) Stan library(rstan) model = &quot; data { int&lt;lower=0&gt; n; // define range and type int&lt;lower=0&gt; a; // and notice semicolons int&lt;lower=0&gt; b; int&lt;lower=0, upper=n&gt; y; } parameters { real&lt;lower=0, upper=1&gt; theta; } model { y ~ binomial(n, theta) ; theta ~ beta(a, b) ; } &quot; dat = list(n=10, y=3, a=1, b=1) m = stan_model(model_code = model) # Only needs to be done once r = sampling(m, data = dat) r ## Inference for Stan model: 7f934cc8d471003538635fa74104a81a. ## 4 chains, each with iter=2000; warmup=1000; thin=1; ## post-warmup draws per chain=1000, total post-warmup draws=4000. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## theta 0.33 0.00 0.13 0.11 0.23 0.32 0.42 0.60 1642 1 ## lp__ -8.16 0.02 0.73 -10.18 -8.33 -7.88 -7.69 -7.64 1663 1 ## ## Samples were drawn using NUTS(diag_e) at Sun Jun 12 20:18:59 2022. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). plot(r) ## ci_level: 0.8 (80% intervals) ## outer_level: 0.95 (95% intervals) "],["multiparameter-models.html", "Chapter 3 Multiparameter Models", " Chapter 3 Multiparameter Models Example 1: Independent Beta-binomial model Assume an independent binomial model, \\[ Y_{s} \\stackrel{i n d}{\\sim} \\operatorname{Bin}\\left(n_{s}, \\theta_{s}\\right) \\text {, i.e. }, p(y \\mid \\theta)=\\prod_{s=1}^{S} p\\left(y_{s} \\mid \\theta_{s}\\right)=\\prod_{s=1}^{S}\\left(\\begin{array}{l} n_{s} \\\\ y_{s} \\end{array}\\right) \\theta_{s}^{y_{s}}\\left(1-\\theta_{s}\\right)^{n_{s}-y_{s}} \\] and assume independent beta priors distribution: \\[ p(\\theta)=\\prod_{s=1}^{S} p\\left(\\theta_{s}\\right)=\\prod_{s=1}^{S} \\frac{\\theta_{s}^{a_{s}-1}\\left(1-\\theta_{s}\\right)^{b_{s}-1}}{\\operatorname{Beta}\\left(a_{s}, b_{s}\\right)} \\mathrm{I}\\left(0&lt;\\theta_{s}&lt;1\\right) \\] Then we have \\(p(\\theta\\mid y) \\propto \\prod_{s = 1}^S \\text{Beta}(\\theta_s\\mid a_s + y_s, b_s + n_s - y_s)\\). Example 2: Normal model with unknown mean and variance Scaled-inverse \\(\\chi^2\\)-distribution: If \\(\\sigma^2 \\sim IG(a, b)\\), then \\(\\sigma^2 \\sim Inv-\\chi^2(v, s^2)\\) where \\(a = v/2\\) and \\(b = vs^2/2\\), or equivalently, \\(v = 2a\\) and \\(s^2 = b/a\\). Location-scale \\(t\\)-distribution: \\(t_v(m, s^2) \\stackrel{v\\rightarrow \\infty}{\\longrightarrow} N(m, s^2)\\). Normal-Inv-\\(\\chi^2\\) distribution: \\(\\mu\\mid \\sigma^2 \\rightarrow N(m, \\sigma^2/k)\\) and \\(\\sigma^2 \\sim Inv-\\chi^2(v, s^2)\\), then the kernel of this joint density is \\[ p(\\mu, \\sigma^2) \\propto (\\sigma^2)^{-(v+3)/2}e^{-\\frac{1}{2\\sigma^2}[k(\\mu - m)^2 + vs^2]} \\] In addition, the marginal distribution for \\(\\mu\\) is \\(t_v(m, s^2/k)\\). Jeffrey prior can be shown to be \\(p(\\mu, \\sigma^2) \\propto (1/\\sigma^2)^{3/2}\\) but reference prior finds that \\(p(\\mu, \\sigma^2) \\propto 1/\\sigma^2\\) is more appropriate. Under the reference prior, the posterior is \\[ p(\\mu \\mid \\sigma^2, y) \\sim N(\\bar y, \\sigma^2/n) \\quad \\sigma^2 \\mid y \\sim \\text{Inv}-\\chi^2(n-1, S^2) \\] and the marginal posterior for \\(\\mu\\) is \\(\\mu\\mid y \\sim t_{n-1}(\\bar y, S^2/n)\\). To predict \\(\\tilde{y} \\sim N(\\mu, \\sigma^2)\\), we can write \\(\\tilde{y} = \\mu + \\epsilon\\) with \\(\\mu\\mid \\sigma^2, y \\sim N(\\bar y, \\sigma^2/n)\\) and \\(\\epsilon\\mid \\sigma^2, y \\sim N(0, \\sigma^2)\\). Thus \\[ \\tilde{y} \\mid \\sigma^2, y \\sim N(\\bar y, \\sigma^2[1+1/n]) \\] Because \\(\\sigma^2\\mid y \\sim \\text{Inv}-\\chi^2(n-1, S^2)\\), we have \\(\\tilde{y}\\mid y \\sim t_{n-1}(\\bar y, S^2[1+1/n])\\). The conjugate prior for \\(\\mu\\) and \\(\\sigma^2\\) is \\[ \\mu \\mid \\sigma^2 \\sim N(m, \\sigma^2/k) \\quad \\sigma^2 \\sim \\text{Inv}-\\chi^2(v, s^2) \\] where \\(s^2\\) serves as a prior guess about \\(\\sigma^2\\) and \\(v\\) controls how certain we are about that guess. The posterior under the prior is \\[ \\mu\\left|\\sigma^{2}, y \\sim N\\left(m^{\\prime}, \\sigma^{2} / k^{\\prime}\\right) \\quad \\sigma^{2}\\right| y \\sim \\operatorname{lnv}-\\chi^{2}\\left(v^{\\prime},\\left(s^{\\prime}\\right)^{2}\\right) \\] where \\(k&#39; = k + n\\), \\(m&#39; = [km + n\\bar y]/k&#39;\\) , \\(v&#39; = v + n\\) and \\(v&#39;(s&#39;)^2 = vs^2 + (n-1)S^2 + \\frac{kn}{k&#39;}(\\bar y - m)^2\\). The marginal posterior for \\(\\mu\\) is \\[ \\mu \\mid y \\sim t_{v&#39;} (m&#39;, (s&#39;)^2/k&#39;) \\] Example 3: Multinomial-Dirichlet Suppose \\(Y = (Y_1, \\ldots, Y_K) \\sim Mult(n,\\pi)\\) with pmf \\(p(y) = n!\\prod_{k=1}^K\\frac{\\pi_k^{y_k}}{y_k!}\\), let \\(\\pi \\sim Dir(a)\\) with concentration parmaeter \\(a = (a_1, \\ldots, a_K)\\) where \\(a_k&gt;0\\) for all \\(k\\). Dirichlet distribution: The pdf of \\(\\pi\\) is \\(p(\\pi) = \\frac{1}{\\text{Beta}(a)}\\prod_{k=1}^K \\pi_k^{a_k - 1}\\) with \\(\\prod_{k=1}^K \\pi_k = 1\\) and \\(Beta(a)\\) is a multinomial beta function, i.e. \\(Beta(a) = \\frac{\\prod_{k=1}^K \\Gamma(a_k)}{\\Gamma(\\sum_{k=1}^K a_k)}\\). \\(E(\\pi_k) = a_k/a_0\\), \\(V(\\pi_k) = a_k(a_0 - a_k)/a_0^2(a_0+ 1)\\) where \\(a_0 = \\sum_{k=1}^K a_k\\). Marginally, each component of a Dirichlet distribution is a Beta distribution with \\(\\pi_k \\sim Be(a_k, a_0 - a_k)\\). The conjugate prior for a multinomial distribution with unknown probabilty vector \\(\\pi\\) is a Dirichlet distribution. The Jeffery prior is a Dirichlet distribution with \\(a_k = 0.5\\) for all \\(k\\). The posterior under a Direchlet prior is \\[ p(\\pi\\mid y) \\propto \\prod_{k=1}^K \\pi_{k}^{a_k + y_k - 1} \\Rightarrow \\pi\\mid y \\sim Dir(a + y) \\] Example 4: Multivariate Normal \\[ p(y) = (2\\pi)^{-k/2}|\\Sigma|^{-1/2}\\exp\\left(-\\frac{1}{2}(y - \\mu)^T \\Sigma^{-1}(y - \\mu) \\right) \\] Let \\(Y \\sim N(\\mu, \\Sigma)\\) with precision matrix \\(\\Omega = \\Sigma^{-1}\\) If \\(\\Sigma_{k, k&#39;} = 0\\), then \\(Y_k\\) and \\(Y_{k&#39;}\\) are independent of each other If \\(\\Omega_{k, k&#39;} = 0\\), then \\(Y_k\\) and \\(Y_{k&#39;}\\) are conditionally independent of each other given \\(Y_j\\) for \\(j \\neq k, k&#39;\\) Conjugate inference: let \\(Y_i \\sim N(\\mu, S^2)\\) with conjugate prior \\(\\mu \\sim N_k(m, C)\\), the posterior \\(\\mu \\mid y \\sim N(m&#39;, C&#39;)\\) where \\(C&#39; = [C^{-1} + nS^{-1}]^{-1}\\) and \\(m&#39; = C&#39;[C^{-1}m + nS^{-1}\\bar y]\\). Let \\(\\Sigma\\) have an inverse Wishart distribution, i.e. \\(\\Sigma \\sim IW(v, W^{-1})\\) with degree of freedom \\(v &gt; K - 1\\) and positive definite scale matrix \\(W\\). A multivariate generalization of the normal-scaled-inverse-\\(\\chi^2\\) distribution is the normal-inverse Wishart distribution. For a vector \\(\\mu \\subset \\mathcal{R}^K\\) and \\(K \\times K\\) matrix \\(\\Sigma\\), the normal-inverse Wishart distribution is \\[ \\mu \\mid \\Sigma \\sim N(m, \\Sigma/c) \\quad \\Sigma \\sim IW(v, W^{-1}) \\] The marginal distribution for \\(\\mu\\) is a multivariate t-distribution, i.e. \\(\\mu \\sim t_{v-K+1}(m, W/[c(v-K+1)])\\). The posterior distribution is \\[ \\mu \\mid \\Sigma, y \\sim N(\\bar y, \\Sigma/n) \\quad \\Sigma \\mid y \\sim IW(n-1, S^{-1}) \\] "],["data-asymptotics.html", "Chapter 4 Data Asymptotics", " Chapter 4 Data Asymptotics Suppose \\(p(\\theta\\mid y)\\) is unimodel and roughly symmetric, a Taylor series expansion of the logarithm of the posterior around the posterior mode \\(\\hat\\theta\\) is \\[ \\log p(\\theta \\mid y)=\\log p(\\hat{\\theta} \\mid y)-\\frac{1}{2}(\\theta-\\hat{\\theta})^{\\top}\\left[-\\frac{d^{2}}{d \\theta^{2}} \\log p(\\theta \\mid y)\\right]_{\\theta=\\hat{\\theta}}(\\theta-\\hat{\\theta})+\\cdots \\] Discarding the higher order terms, this expansionsion provides a normal approximation to the posterior, i.e. \\(p(\\theta\\mid y) \\stackrel{d}{\\approx} N(\\hat\\theta, I(\\hat\\theta)^{-1})\\). Theorem 1: If the parameter space \\(\\Theta\\) is discrete and \\(P(\\theta = \\theta_0) &gt; 0\\), then \\(P(\\theta = \\theta_0\\mid y) \\rightarrow 1\\) as \\(n \\rightarrow \\infty\\). Theorem 2: If the parameter space \\(\\Theta\\) is continuous and \\(A\\) is a neighborhood around \\(\\Theta_0\\) with \\(P(\\theta\\in A) &gt; 0\\), then \\(P(\\theta \\in A \\mid y) \\rightarrow 1\\) as \\(n \\rightarrow \\infty\\). An estimator is consistent, i.e. \\(\\hat\\theta \\stackrel{p}{\\rightarrow} \\theta_0\\) if \\(\\lim_{n \\rightarrow \\infty} P(|\\hat\\theta - \\theta_0| &lt; \\epsilon) = 1\\). Under regularity conditions, \\(\\hat\\theta_{MLE} \\stackrel{p}{\\rightarrow} \\theta_0\\) . Example 1: Binomial example Let \\(y \\sim Bin(n, \\theta)\\) and \\(\\theta \\sim Be(a, b)\\), then \\(\\theta\\mid y \\sim Be(a + y, b + n - y)\\) and the posterior mode is \\(\\hat\\theta = \\frac{y&#39;}{n&#39;} = \\frac{a + y - 1}{a + b + n - 2}\\). Thus \\(I(\\hat\\theta) = \\frac{n&#39;}{\\hat\\theta(1 - \\hat\\theta)}\\) and \\(p(\\theta\\mid y) \\stackrel{d}{\\approx } N\\left(\\hat\\theta, \\frac{\\hat\\theta(1 - \\hat\\theta)}{n&#39;} \\right)\\). Recall that \\(\\hat\\theta_{MLE} = y/n\\). The following estimators are all consistent Posterior mean: \\(\\frac{a + y}{a + b + n}\\) Posterior median: \\(\\approx \\frac{a + y - 1/3}{a + b + n - 2/3}\\) for \\(a, b &gt; 1\\) Posterior mode: \\(\\frac{a + y - 1}{a + b + n - 2}\\) since as \\(n \\rightarrow \\infty\\), these all converage to \\(\\hat\\theta_{MLE} = y/n\\). Example 2: Normal example Consider \\(Y_i \\stackrel{iid}{\\sim} N(\\theta, 1)\\) with known and prior \\(\\theta \\sim N(c, 1)\\), then \\(\\theta\\mid y \\sim N\\left(\\frac{1}{n+1}c + \\frac{n}{n+1}\\bar y, \\frac{1}{n+1} \\right)\\). Recall that \\(\\hat\\theta_{MLE} = \\bar y\\), and the posterior mean coverages to the MLE. Asymptotic normality For large \\(n\\), we have \\[ \\log p(\\theta \\mid y) \\approx \\log p(\\hat{\\theta} \\mid y)-\\frac{1}{2}(\\theta-\\hat{\\theta})^{\\top}\\left[n \\mathrm{I}\\left(\\theta_{0}\\right)\\right](\\theta-\\hat{\\theta}) \\] where \\(\\hat\\theta\\) is the posterior mode. Since \\(\\hat\\theta \\rightarrow \\theta_0\\) and \\(I(\\hat\\theta) \\rightarrow I(\\theta_0)\\) as \\(n \\rightarrow \\infty\\), we have \\[ p(\\theta\\mid y )\\propto \\exp\\left(-\\frac{1}{2}(\\theta - \\hat\\theta)^T\\left[n I(\\hat\\theta) \\right] (\\theta - \\hat\\theta) \\right). \\] Thus, \\(\\theta\\mid y \\stackrel{d}{\\rightarrow} N\\left(\\hat\\theta, \\frac{1}{n} I(\\hat\\theta)^{-1} \\right)\\), i.e. the posterior distribution is asymptotically normal. Suppose that \\(f(y)\\) the true sampling distribution does not correspond to \\(p(y\\mid \\theta)\\) for some \\(\\theta = \\theta_0\\). Then the posterior \\(p(\\theta\\mid y)\\) converges to a \\(\\theta_0\\) that is the smallest in Kullback-Leibler divergence to the true \\(f(y)\\) where \\[ K L(f(y) \\| p(y \\mid \\theta))=E\\left[\\log \\left(\\frac{f(y)}{p(y \\mid \\theta)}\\right)\\right]=\\int \\log \\left(\\frac{f(y)}{p(y \\mid \\theta)}\\right) f(y) d y \\] That is, we do about the best that we can given that we have assumed the wrong sampling distribution \\(p(y \\mid \\theta)\\). "],["hierarchical-model.html", "Chapter 5 Hierarchical Model", " Chapter 5 Hierarchical Model Consider the following model: \\[ \\begin{aligned} y_i &amp; \\stackrel{ind}{\\sim} p(y\\mid \\theta_i)\\\\ \\theta_i &amp; \\stackrel{ind}{\\sim} p(\\theta\\mid \\phi) \\\\ \\phi &amp;\\sim p(\\phi) \\end{aligned} \\] This is a hierarchical or multilevel model. The joint posterior distribution can be decomposed via \\[ p(\\theta, \\phi\\mid y) = p(\\theta\\mid \\phi, y)p(\\phi \\mid y) \\] where \\(p(\\theta\\mid \\phi, y) \\propto p(y\\mid \\theta)p(\\theta\\mid \\phi)= \\prod_{i = 1}^n p(y_i \\mid \\theta_i)p(\\theta_i \\mid \\phi) = \\prod_{i = 1}^n p(\\theta_i \\mid \\phi, y_i)\\) $p(y) p(y) p() $ \\(p(y \\mid \\phi) = \\int p(y\\mid \\theta) p(\\theta\\mid \\phi)d\\theta = \\prod_{i = 1}^n \\int p(y_i\\mid \\theta_i)p(\\theta_i\\mid \\phi) d\\theta_i = \\prod_{i = 1}^n p(y_i \\mid \\phi)\\) Example: Beta-Binomial Consider \\[ Y_i \\stackrel{ind}{\\sim} Bin(n_i, \\theta_i) \\quad \\theta_i \\stackrel{ind}{\\sim} Be(\\alpha, \\beta) \\quad \\alpha, \\beta \\sim p(\\alpha, \\beta) \\] In this example, \\(\\phi = (\\alpha, \\beta)\\). \\(p(\\theta \\mid \\alpha, \\beta, y) = \\prod_{i = 1}^n Be(\\theta_i\\mid \\alpha + y_i, \\beta + n_i - y_i)\\). The marginal posterior for \\(\\alpha, \\beta\\) is \\(p(\\alpha, \\beta \\mid y) \\propto p(y\\mid \\alpha, \\beta) p(\\alpha, \\beta)\\) with \\[ \\begin{aligned} p(y\\mid \\alpha, \\beta) &amp;= \\prod_{i = 1}^n \\int p(y_i\\mid \\theta_i) p(\\theta_i\\mid \\alpha, \\beta) d\\theta_i \\\\ &amp; = \\prod_{i = 1}^n \\int Bin(y_i\\mid n_i, \\theta_i)Be(\\theta_i \\mid \\alpha, \\beta)d\\theta_i \\\\ &amp; = \\prod_{i = 1}^n {n_i\\choose y_i}\\frac{B(\\alpha + y_i , \\beta + n_i - y_i)}{B(\\alpha, \\beta)} \\end{aligned} \\] Thus, \\(y_i \\mid \\alpha, \\beta \\stackrel{ind}{\\sim} \\text{Beta-binomial}(n_i, \\alpha, \\beta)\\). \\(\\alpha\\): prior successes and \\(\\beta\\): prior failures A more natural parameterization is prior expectation: \\(\\mu = \\frac{\\alpha}{\\alpha + \\beta}\\) prior sample size \\(\\eta = \\alpha + \\beta\\) We can assume the mean (\\(\\mu\\)) and the size (\\(\\eta\\)) are independent a priori. For example, suppose \\(\\mu \\sim Be(20, 30)\\) and \\(\\eta \\sim LN(0, 3^2)\\) where \\(LN\\) is short for log-normal distribution. model_informative_prior = &quot; data { int&lt;lower=0&gt; N; // data int&lt;lower=0&gt; n[N] ; int&lt;lower=0&gt; y[N]; real&lt;lower=0&gt; a; // prior real&lt;lower=0&gt; b; real&lt;lower=0&gt; C; real m; } parameters { real&lt;lower=0, upper=1&gt; mu; real&lt;lower=0&gt; eta; real&lt;lower=0, upper=1&gt; theta[N]; } transformed parameters { real&lt;lower=0&gt; alpha; real&lt;lower=0&gt; beta; alpha = eta* mu; beta = eta*(1-mu) ; } model { mu ~ beta(a,b); eta ~ lognormal(m,C); # implicit joint distributions theta ~ beta(alpha,beta); y ~ binomial(n, theta); } &quot; a = 20; b = 30; m = 0; C = 3 n = 1e4 prior_draws = mutate(data.frame(mu = rbeta(n, a, b), eta = rlnorm(n, m, C)), alpha = eta* mu, beta = eta*(1-mu)) dat = list(y=d$made, n=d$attempts, N=nrow(d), a=a, b=b, m=m, C=C) m = stan_model(model_code=model_informative_prior) r = sampling(m, dat, c(&quot;mu&quot;, &quot;eta&quot;, &quot;alpha&quot; &quot;beta&quot;, &quot;theta&quot;), iter = 10000) plot(r, pars = c(&#39;eta&#39;, &#39;alpha&#39;, &#39;beta&#39;)) plot(r, pars = c(&#39;mu&#39;, &#39;theta&#39;)) In the Bayesian Data Analysis page 110, serveral priors are discussed: \\((\\log(\\alpha/\\beta), \\log(\\alpha + \\beta)) \\propto 1\\) leads to an improper posterior \\((\\log(\\alpha/\\beta), \\log(\\alpha + \\beta)) \\sim Unif([-10^{10}, 10^{10}]\\times [-10^{10}, 10^{10}])\\) while proper and seemingly vague is a very informative prior \\((\\log(\\alpha, \\beta), \\log(\\alpha+\\beta))\\propto \\alpha\\beta(\\alpha+\\beta)^{-5/2}\\) while leads to a proper posterior and is equivalent to \\(p(\\alpha, \\beta) \\propto (\\alpha + \\beta)^{-5/2}\\) # Stan code for default prior model_default_prior = &quot; data { int&lt;lower=0&gt; N; int&lt;lower=0&gt; n [N]; int&lt;lower=0&gt; y [N]; } parameters { real&lt;lower=0&gt; alpha; real&lt;lower=0&gt; beta; real&lt;lower=0, upper=1&gt; theta[N]; } model { # default prior target += -5 * log(alpha+beta)/2; # implicit joint distributions theta ~ beta(alpha, beta) ; y ~ binomial(n, theta); } &quot; An alternative to jointly sampling \\(\\theta, \\alpha, \\beta\\) is to sample \\(\\alpha, \\beta \\sim p(\\alpha, \\beta\\mid y)\\), and then sample \\(\\theta \\stackrel{ind}{\\sim} p(\\theta_i \\mid \\alpha, \\beta, y_i) \\stackrel{d}{=} Be(\\alpha + y_i, \\beta + n_i - y_i)\\). The marginal posterior for \\(\\alpha, \\beta\\) is \\[ p(\\alpha, \\beta\\mid y) \\propto p(y\\mid \\alpha, \\beta)p(\\alpha, \\beta) = \\left[\\prod_{i = 1}^n \\text{Beta-Binomial}(y_i \\mid n_i, \\alpha, \\beta) \\right]p(\\alpha, \\beta) \\] # Marginalized (integrated) theta out of the model model_marginalized = &quot; data { int&lt;lower=0&gt; N; int&lt;lower=0&gt; n[N]; int&lt;lower=0&gt; y[N]; } parameters { real&lt;lower=0&gt; alpha; real&lt;lower=0&gt; beta; } model { target += -5 * log(alpha+beta)/2; y ~ beta_binomial(n, alpha, beta); } &quot; The forth approach: # Stan Beta-Binomial model_seasons = &quot; data { int&lt;lower=0&gt; N; int&lt;lower=0&gt; n[N]; int&lt;lower=0&gt; y[N] ; real&lt;lower=0&gt; a; real&lt;lower=0&gt; b; real&lt;lower=0&gt; C; real m; } parameters { real&lt;lower=0, upper=1&gt; mu; real&lt;lower=0&gt; eta; } transformed parameters { real&lt;lower=0&gt; alpha; real&lt;lower=0&gt; beta; alpha = eta * mu; beta = eta * (1-mu) ; } model { mu ~ beta(a,b); eta ~ lognormal(m,C); y ~ beta_binomial(n, alpha, beta); } generated quantities { real&lt;lower=0, upper=1&gt; theta[N] ; for (i in 1:N) theta [i] = beta_rng(alpha+y[i], beta + n[i]-y[i]); } &quot; Exchangeability: The set \\(Y_1, Y_2, \\ldots, Y_n\\) is exchangeable if the joint probabilty \\(p(y_1, \\ldots, y_n)\\) is invariant to permutation of the indices. That is, for any permutation \\(\\pi\\), \\(p(y_1, \\ldots, y_n) = p(y_{\\pi_1}, \\ldots, y_{\\pi_n})\\). Theorem 1: All independent and identically distributed random variables are exchangeable. Theorem 2 (de Finetti’s Theorem): A sequence of random variables \\((y_1, y_2, \\ldots)\\) is infinitely exchangeable iff, for all \\(n\\), \\[ p(y_1, \\ldots, y_n) = \\int \\prod_{i = 1}^n p(y_i\\mid \\theta)P(d\\theta) \\] for some measure \\(P\\) on \\(\\theta\\). Although hierarchical models are typically written using the conditional independence notation above, the assumptions underlying the model are exchangeability and functional forms for the priors. \\[ y_i \\stackrel{ind}{\\sim} p(y\\mid \\theta_i) \\quad \\theta_i \\stackrel{ind}{\\sim} p(\\theta\\mid \\phi) \\quad \\phi \\sim p(\\phi) \\] Example 2: Normal hierarchical model Suppose we have the following model: \\[ \\begin{aligned} y_{ij} &amp; \\sim N(\\theta_i, \\sigma^2)\\\\ \\theta_i &amp; \\sim N(\\mu, \\tau^2) \\end{aligned} \\] with \\(j = 1,\\ldots, n_i\\), \\(i = 1, \\ldots, I\\) and \\(n = \\sum_{i = 1}^I n_i\\). We assume \\(\\sigma^2 = s^2\\) is known and \\(p(\\mu, \\tau^2) \\propto p(\\tau)\\), i.e. assume an improper uniform prior on \\(\\mu\\). (Section 5.4 in Bayesian Data Analysis) Let \\(\\bar y_{i.} = \\frac{1}{n_i} \\sum_{i = 1}^{n_i} y_{ij}\\) and \\(s^2_i = s^2/n_{i}\\), then \\[ \\left\\{\\begin{aligned} p(\\tau \\mid y) &amp; \\propto p(\\tau) V_{\\mu}^{1 / 2} \\prod_{i=1}^{\\mathrm{I}}\\left(s_{i}^{2}+\\tau^{2}\\right)^{-1 / 2} \\exp \\left(-\\frac{\\left(\\bar{y}_{i}-\\hat{\\mu}\\right)^{2}}{2\\left(s_{i}^{2}+\\tau^{2}\\right)}\\right) \\\\ \\mu \\mid \\tau, y &amp; \\sim N\\left(\\hat{\\mu}, V_{\\mu}\\right) &amp; \\\\ \\theta_{i} \\mid \\mu, \\tau, y &amp; \\sim N\\left(\\hat{\\theta}_{i}, V_{i}\\right) &amp; \\\\ \\end{aligned}\\right. \\] with \\[ V_{\\mu}^{-1} =\\sum_{j=1}^{J} \\frac{1}{s_{i}^{2}+\\tau^{2}} \\quad \\hat{\\mu}=V_{\\mu}\\left(\\sum_{i=1}^{\\mathrm{I}} \\frac{\\bar{y}_{i}}{s_{i}^{2}+\\tau^{2}}\\right) \\\\ V_{i}^{-1} =\\frac{1}{s_{i}^{2}}+\\frac{1}{\\tau^{2}} \\quad \\hat{\\theta}_{i} =V_{i}\\left(\\frac{\\bar{y}_{i}}{s_{i}^{2}}+\\frac{\\mu}{\\tau^{2}}\\right) \\] One choice for \\(p(\\tau)\\) is \\(Ca^+(0, 1)\\) There are some alternative distributions for \\(\\theta_i\\): Heavy-tailed: \\(\\theta_i \\sim t_{\\nu}(\\mu, \\tau^2)\\) Peak at zero: \\(\\theta_i \\sim Laplace(\\mu, \\tau^2)\\) Point mass at zero: \\(\\theta_i \\sim \\pi\\delta_0 + (1 - \\pi)N(\\mu, \\tau^2)\\) Hierarchical models allow the data to inform us about similarities across groups provide data driven shrinkage toward a grand mean lots of shrinkage when means are similar little shrinkage when means are different "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
